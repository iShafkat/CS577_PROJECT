{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xU-eMjCpwp2D"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "import string\n",
        "from keras_tuner import RandomSearch\n",
        "import numpy as np \n",
        "import random\n",
        "import pandas as pd \n",
        "from keras.layers import (LSTM,Embedding,BatchNormalization,Dense,TimeDistributed,Dropout,Bidirectional,Flatten,GlobalMaxPool1D,GlobalAveragePooling1D,MultiHeadAttention,LayerNormalization,SimpleRNN)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "from plotly import graph_objs as go\n",
        "import plotly.express as px\n",
        "import plotly.figure_factory as ff\n",
        "from plotly.offline import iplot\n",
        "from PIL import Image\n",
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
        "import nltk\n",
        "from lime.lime_text import LimeTextExplainer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import nltk\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.initializers import Constant\n",
        "from keras.layers import (LSTM, \n",
        "                          Embedding, \n",
        "                          BatchNormalization,\n",
        "                          Dense, \n",
        "                          TimeDistributed, \n",
        "                          Dropout, \n",
        "                          Bidirectional,\n",
        "                          Flatten, \n",
        "                          GlobalMaxPool1D)\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
        "from keras.optimizers import Adam\n",
        "from sklearn.metrics import (\n",
        "    precision_score, \n",
        "    recall_score, \n",
        "    f1_score, \n",
        "    classification_report,\n",
        "    accuracy_score\n",
        ")\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from keras.utils import pad_sequences\n",
        "from sklearn import metrics\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\n",
        "from keras.models import Model\n",
        "from keras.optimizers import RMSprop\n",
        "\n",
        "from keras.callbacks import EarlyStopping"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "r0JUIsj6xx4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"spam.csv\", delimiter=',',encoding='latin-1')\n",
        "df.drop(['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'],axis=1,inplace=True)\n",
        "df.rename(columns = {\"v1\": \"target\", \"v2\": \"text\"}, inplace = True)\n",
        "x = ['ham', 'spam']\n",
        "y = df.groupby(\"target\")[\"target\"].agg(\"count\").values\n",
        "df['text_len'] = df['text'].apply(lambda x: len(x.split(' ')))"
      ],
      "metadata": {
        "id": "IZdk7S34x0g7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "layout = go.Layout(title={'text':'Proportional Distribution of the Target Variable',\n",
        "                         'y':0.9,\n",
        "                         'x':0.5,\n",
        "                         'xanchor':'center',\n",
        "                         'yanchor':'top'},\n",
        "                  template = 'plotly_dark')\n",
        "\n",
        "fig = go.Figure(data=[go.Bar(\n",
        "    x = x, y = y,\n",
        "    text = y, textposition = 'auto',\n",
        "    marker_color = \"slateblue\"\n",
        ")], layout = layout)\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "6356L1PCyDZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "colors = [\"slateblue\", \"darkred\"]\n",
        "\n",
        "fig = go.Figure(data=[go.Pie(labels = df['target'].value_counts().keys(),\n",
        "                             values = df['target'].value_counts().values,\n",
        "                             pull = [0, 0.25])])\n",
        "\n",
        "fig.update_traces(hoverinfo ='label',\n",
        "                  textinfo ='percent',\n",
        "                  textfont_size = 20,\n",
        "                  textposition ='auto',\n",
        "                  marker=dict(colors=colors,\n",
        "                              line = dict(color = 'lightgray',\n",
        "                                          width = 1.5)))\n",
        "fig.update_layout(title={'text': \"Percentages of the Target Values\",\n",
        "                         'y':0.9,\n",
        "                         'x':0.5,\n",
        "                         'xanchor': 'center',\n",
        "                         'yanchor': 'top'},\n",
        "                  template='plotly_dark')\n",
        "\n",
        "iplot(fig)"
      ],
      "metadata": {
        "id": "NyvPyMS-yFkC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ham = df[df[\"target\"] == \"ham\"][\"text_len\"].value_counts().sort_index()\n",
        "spam = df[df[\"target\"] == \"spam\"][\"text_len\"].value_counts().sort_index()\n",
        "fig = go.Figure()\n",
        "fig.add_trace(go.Scatter(x = ham.index, y = ham.values, name = \"ham\", \n",
        "                         fill = \"tozeroy\"))\n",
        "fig.add_trace(go.Scatter(x = spam.index, y = spam.values, name=\"spam\",\n",
        "                        fill = \"tozeroy\"))\n",
        "fig.update_layout(title={'text': \"Distributions of Target Values\",\n",
        "                         'y':0.9,\n",
        "                         'x':0.5,\n",
        "                         'xanchor': 'center',\n",
        "                         'yanchor': 'top'},\n",
        "                  template='plotly_dark')\n",
        "fig.update_xaxes(range=[0, 50])\n",
        "fig.update_yaxes(range=[0, 450])\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "uG9giV4myMKR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    text = str(text).lower()\n",
        "    text = re.sub('\\[.*?\\]', '', text)\n",
        "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
        "    text = re.sub('<.*?>+', '', text)\n",
        "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
        "    text = re.sub('\\n', '', text)\n",
        "    text = re.sub('\\w*\\d\\w*', '', text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "FuE1CuNgyPMd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['text'] = df['text'].apply(clean_text)\n",
        "nltk.download('stopwords')\n",
        "stop_words = stopwords.words(\"english\")\n",
        "df[\"text\"] = df[\"text\"].apply(lambda x: \" \".join(x for x in x.split() if x not in stop_words))\n",
        "stemmer = nltk.SnowballStemmer(\"english\")\n",
        "df[\"text\"] = df[\"text\"].apply(lambda x: \" \".join([stemmer.stem(word) for word in x.split()]))\n",
        "text = \" \".join(i for i in df.text)\n",
        "\n",
        "wc = WordCloud(background_color = \"black\", width = 1200, height = 600,\n",
        "               contour_width = 0, contour_color = \"#410F01\", max_words = 1000,\n",
        "               scale = 1, collocations = False, repeat = True, min_font_size = 1)\n",
        "\n",
        "wc.generate(text)\n",
        "lb = LabelEncoder()\n",
        "df[\"target\"] = lb.fit_transform(df[\"target\"])"
      ],
      "metadata": {
        "id": "bAalDT7yyRkV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df['text']\n",
        "y = df['target']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
        "\n",
        "vec = CountVectorizer()\n",
        "vec.fit(X_train)\n",
        "\n",
        "X_train_dtm = vec.transform(X_train)\n",
        "X_test_dtm = vec.transform(X_test)\n",
        "max_words = 1000\n",
        "max_len = 150\n",
        "tok = Tokenizer(num_words=max_words)\n",
        "tok.fit_on_texts(X_train)\n",
        "sequences = tok.texts_to_sequences(X_train)\n",
        "sequences_matrix = pad_sequences(sequences,maxlen=max_len)"
      ],
      "metadata": {
        "id": "hx7EgBbWzPQi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test1 = y_test\n",
        "X_test1 = X_test\n",
        "try:\n",
        "  while y_test1[y_test1 == 0].index[0]:\n",
        "    index_30 = y_test1[y_test1 == 0].index[0]\n",
        "    y_test1.drop(index_30, inplace = True)\n",
        "    X_test1.drop(index_30, inplace = True)\n",
        "except: pass\n"
      ],
      "metadata": {
        "id": "fO4BeeGRGEQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def LSTM_model():\n",
        "    inputs = Input(name='inputs',shape=[max_len])\n",
        "    layer = Embedding(max_words,50,input_length=max_len)(inputs)\n",
        "    layer = LSTM(64)(layer)\n",
        "    layer = Dense(256,name='FC1')(layer)\n",
        "    layer = Activation('relu')(layer)\n",
        "    layer = Dropout(0.5)(layer)\n",
        "    layer = Dense(1,name='out_layer')(layer)\n",
        "    layer = Activation('sigmoid')(layer)\n",
        "    model = Model(inputs=inputs,outputs=layer)\n",
        "    return model"
      ],
      "metadata": {
        "id": "BaFP1orkziKh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def RNN_model():\n",
        "    inputs = Input(name='inputs',shape=[max_len])\n",
        "    layer = Embedding(max_words,50,input_length=max_len)(inputs)\n",
        "    layer = SimpleRNN(64)(layer)\n",
        "    layer = Dense(256,name='FC1')(layer)\n",
        "    layer = Activation('relu')(layer)\n",
        "    layer = Dropout(0.5)(layer)\n",
        "    layer = Dense(1,name='out_layer')(layer)\n",
        "    layer = Activation('sigmoid')(layer)\n",
        "    model = Model(inputs=inputs,outputs=layer)\n",
        "    return model"
      ],
      "metadata": {
        "id": "9tgMcUrniM0t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def DNN_model():\n",
        "    inputs = Input(name='inputs',shape=[max_len])\n",
        "    layer = Embedding(max_words,50,input_length=max_len)(inputs)\n",
        "    layer = Dense(64)(layer)\n",
        "    layer = Dense(256,name='FC1')(layer)\n",
        "    layer = Activation('relu')(layer)\n",
        "    layer = Dropout(0.5)(layer)\n",
        "    layer = Dense(1,name='out_layer')(layer)\n",
        "    layer = Activation('sigmoid')(layer)\n",
        "    model = Model(inputs=inputs,outputs=layer)\n",
        "    return model"
      ],
      "metadata": {
        "id": "SPga_HlWIMh2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_rnn = RNN_model()\n",
        "model_rnn.summary()\n",
        "model_rnn.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "yJFB6pQ8Tm_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_dnn = DNN_model()\n",
        "model_dnn.summary()\n",
        "model_dnn.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "kklyAnUIVcyE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_lstm = LSTM_model()\n",
        "model_lstm.summary()\n",
        "model_lstm.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "_sRHtA_QzkYd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_lstm.fit(sequences_matrix,y_train,batch_size=128,epochs=10,\n",
        "          validation_split=0.2,callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.0001)])"
      ],
      "metadata": {
        "id": "I6EFENXszmIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_rnn.fit(sequences_matrix,y_train,batch_size=128,epochs=10,\n",
        "          validation_split=0.2,callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.0001)])"
      ],
      "metadata": {
        "id": "hMoYQrmuTurf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_dnn.fit(sequences_matrix,y_train,batch_size=128,epochs=10,\n",
        "          validation_split=0.2,callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.0001)])"
      ],
      "metadata": {
        "id": "kBb-xdmMVlIi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_sequences = tok.texts_to_sequences(X_test1)\n",
        "test_sequences_matrix = pad_sequences(test_sequences,maxlen=max_len)\n",
        "y_pred_dnn = model_dnn.predict(test_sequences_matrix)\n",
        "\n",
        "accr = model_dnn.evaluate(test_sequences_matrix,y_test1)\n"
      ],
      "metadata": {
        "id": "uMZjWVguVo8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_sequences = tok.texts_to_sequences(X_test1)\n",
        "test_sequences_matrix = pad_sequences(test_sequences,maxlen=max_len)\n",
        "y_pred_lstm = model_lstm.predict(test_sequences_matrix)\n",
        "\n",
        "accr = model_lstm.evaluate(test_sequences_matrix,y_test1)\n"
      ],
      "metadata": {
        "id": "rN9M95rPzoeH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_sequences = tok.texts_to_sequences(X_test1)\n",
        "test_sequences_matrix = pad_sequences(test_sequences,maxlen=max_len)\n",
        "y_pred_rnn = model_rnn.predict(test_sequences_matrix)\n",
        "\n",
        "accr = model_rnn.evaluate(test_sequences_matrix,y_test1)"
      ],
      "metadata": {
        "id": "aAKRBXC7TysM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Define the Transformer model with tunable hyperparameters\n",
        "def build_model(hp):\n",
        "    input_dim = max_words\n",
        "    output_dim = hp.Int('output_dim', min_value=32, max_value=128, step=32)\n",
        "    num_heads = hp.Int('num_heads', min_value=2, max_value=8, step=2)\n",
        "    dff = hp.Int('dff', min_value=32, max_value=128, step=32)\n",
        "    rate = hp.Float('rate', min_value=0.1, max_value=0.5, step=0.1)\n",
        "\n",
        "    inputs = Input(shape=(max_len,))\n",
        "    embedding_layer = Embedding(input_dim, output_dim, input_length=max_len)(inputs)\n",
        "    attn_output = MultiHeadAttention(num_heads=num_heads, key_dim=output_dim)(embedding_layer, embedding_layer)\n",
        "    attn_output = LayerNormalization(epsilon=1e-6)(embedding_layer + attn_output)\n",
        "    ffn_output = Dense(dff, activation='relu')(attn_output)\n",
        "    ffn_output = Dense(output_dim)(ffn_output)\n",
        "    ffn_output = LayerNormalization(epsilon=1e-6)(attn_output + ffn_output)\n",
        "    pooling = GlobalAveragePooling1D()(ffn_output)\n",
        "    outputs = Dense(1, activation='sigmoid')(pooling)\n",
        "    \n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    model.compile(optimizer=Adam(learning_rate=rate), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# 5. Set up Keras Tuner and perform hyperparameter tuning\n",
        "tuner = RandomSearch(\n",
        "    build_model,\n",
        "    objective='val_accuracy',\n",
        "    max_trials=1,\n",
        "    executions_per_trial=1,\n",
        "    directory='sms_spam_tuner',\n",
        "    project_name='sms_spam_transformer'\n",
        ")\n",
        "\n",
        "tuner.search_space_summary()\n",
        "\n",
        "tuner.search(sequences_matrix,y_train, epochs=1, validation_split=0.2,callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.0001)])\n",
        "\n",
        "#6. Get the best model and its hyperparameters\n",
        "\n",
        "best_model = tuner.get_best_models(num_models=1)[0]\n",
        "best_hyperparameters = tuner.get_best_hyperparameters(num_trials=1)[0]"
      ],
      "metadata": {
        "id": "ASeL6rvFfPu1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = best_model.fit(sequences_matrix, y_train, batch_size=32, epochs=1, validation_split=0.2)\n"
      ],
      "metadata": {
        "id": "PxHETWjoMeZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_sequences = tok.texts_to_sequences(X_test1)\n",
        "test_sequences_matrix = pad_sequences(test_sequences,maxlen=max_len)\n",
        "y_pred_transformer = best_model.predict(test_sequences_matrix)\n",
        "accr = best_model.evaluate(test_sequences_matrix,y_test1)\n"
      ],
      "metadata": {
        "id": "vT82d9PnPM5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count = 0\n",
        "for i in range(len(y_pred_lstm)):\n",
        "  val = [0,0,0]\n",
        "  if y_pred_lstm[i]>=0.5:\n",
        "    val[0] = 1\n",
        "  try:\n",
        "    if y_pred_dnn[i]>=0.5:\n",
        "      val[1] = 1\n",
        "  except: pass  \n",
        "  if y_pred_rnn[i]>=0.5:\n",
        "    val[2] = 1\n",
        "  if val.count(1)>=2:\n",
        "    count = count+1\n",
        "\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "jPDaPZjVUOtS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = [\"ham\", \"spam\"]\n",
        "explainer = LimeTextExplainer(class_names = class_names)\n",
        "train_df = df\n",
        "\n",
        "## split to train and val\n",
        "train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=2018)\n",
        "val_df.reset_index(drop=True)\n",
        "\n",
        "## vectorize to tf-idf vectors\n",
        "tfidf_vc = TfidfVectorizer(min_df = 10, max_features = 100000, analyzer = \"word\", ngram_range = (1, 2), stop_words = 'english', lowercase = True)\n",
        "train_vc = tfidf_vc.fit_transform(train_df[\"text\"])\n",
        "val_vc = tfidf_vc.transform(val_df[\"text\"])\n",
        "spam_index = y_test1.index[y_test1 == 1]\n",
        "spam_index = list(spam_index)"
      ],
      "metadata": {
        "id": "Q9I-SBgg9pNe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "idx = spam_index[11]\n",
        "model_adv = LogisticRegression(C = 0.5, solver = \"sag\")\n",
        "model_adv = model_adv.fit(train_vc, train_df.target)\n",
        "val_pred = model_adv.predict(val_vc)\n",
        "\n",
        "\n",
        "val_cv = f1_score(val_df.target, val_pred, average = \"binary\")\n",
        "print(val_cv)\n",
        "\n",
        "\n",
        "c = make_pipeline(tfidf_vc, model_adv)\n",
        "class_names = [\"ham\", \"spam\"]\n",
        "explainer = LimeTextExplainer(class_names = class_names)\n",
        "exp = explainer.explain_instance(df[\"text\"][idx], c.predict_proba)\n",
        "exp.as_list()"
      ],
      "metadata": {
        "id": "o0VJQEZU-S-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(idx)\n",
        "X_test1[idx] = \"winner value network custom select receive a reward call claim code valid hour\"\n",
        "test_sequences = tok.texts_to_sequences(X_test1)\n",
        "test_sequences_matrix = pad_sequences(test_sequences,maxlen=max_len)\n",
        "y_pred = model.predict(test_sequences_matrix)\n",
        "y_pred[11]"
      ],
      "metadata": {
        "id": "vvmKls0m_DKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test1 = X_test1.str.replace('call', 'phone')\n",
        "words = X_test1.str.split(expand=True).stack()\n",
        "word_freq = words.value_counts()"
      ],
      "metadata": {
        "id": "5mIApvA3YJeF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test1 = X_test1.str.replace('txt', 'text')\n",
        "words = X_test1.str.split(expand=True).stack()\n",
        "word_freq = words.value_counts()\n",
        "word_freq"
      ],
      "metadata": {
        "id": "WmsgUFQ5ZRUX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test1 = X_test1.str.replace('å£', ' ')\n",
        "X_test1 = X_test1.str.replace('optin', 'option')\n",
        "X_test1 = X_test1.str.replace('mobilee', 'mobile')\n",
        "X_test1 = X_test1.str.replace('repli', 'reply')\n",
        "X_test1 = X_test1.str.replace('claim', ' ')\n",
        "X_test1 = X_test1.str.replace('servic', 'service')\n",
        "X_test1 = X_test1.str.replace('prize', ' ')\n",
        "words = X_test1.str.split(expand=True).stack()\n",
        "word_freq = words.value_counts()\n",
        "word_freq.head(10)"
      ],
      "metadata": {
        "id": "2CgneaROcIWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test1 = X_test1.str.replace('text', ' ')\n",
        "word_freq.head(20)"
      ],
      "metadata": {
        "id": "xZVG41N2dR3z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test1[1044]=\"know someon know deluxe call find\"\n",
        "X_test1[683] = \"hi i am sue year old lapdanc love sex live im bedroom text sue textoper\""
      ],
      "metadata": {
        "id": "3UoVBONRUYJ8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}